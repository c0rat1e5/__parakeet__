#!/usr/bin/env python3
"""
ğŸ¦œ Parakeet TDT-CTC 0.6B-ja Web UI
ãƒ­ãƒ¼ã‚«ãƒ«ãƒ›ã‚¹ãƒˆã§å‹•ä½œã™ã‚‹æ—¥æœ¬èªéŸ³å£°æ›¸ãèµ·ã“ã—Webã‚¢ãƒ—ãƒª

ä½¿ç”¨æ–¹æ³•:
    python app.py
    
ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:7860 ã‚’é–‹ã
"""

from nemo.collections.asr.models import ASRModel
import torch
import gradio as gr
import gc
import shutil
from pathlib import Path
from pydub import AudioSegment
import numpy as np
import os
import tempfile
import csv
import datetime
import subprocess

# ========================================
# è¨­å®š
# ========================================
SCRIPT_DIR = Path(__file__).parent.resolve()
MODEL_PATH = SCRIPT_DIR / "parakeet-tdt_ctc-0.6b-ja.nemo"
TEMP_DIR = SCRIPT_DIR / "temp"

# CUDAã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã™ã‚‹ãŸã‚CPUã‚’ä½¿ç”¨ï¼ˆå®‰å®šæ€§å„ªå…ˆï¼‰
# GPUã‚’ä½¿ç”¨ã—ãŸã„å ´åˆã¯ "cuda" ã«å¤‰æ›´
device = "cpu"
# device = "cuda" if torch.cuda.is_available() else "cpu"

# ãƒ¢ãƒ‡ãƒ«ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«èª­ã¿è¾¼ã¿
print(f"ğŸ¦œ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {MODEL_PATH}")
print(f"   ãƒ‡ãƒã‚¤ã‚¹: {device}")

if not MODEL_PATH.exists():
    raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {MODEL_PATH}")

model = ASRModel.restore_from(str(MODEL_PATH))
model.eval()

# èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’GPUã«ç§»å‹•ã—ã¦ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
if device == "cuda":
    model = model.to(device)
    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼šCUDAã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆæœŸåŒ–
    with torch.no_grad():
        dummy = torch.zeros(1, 16000, device=device)
        del dummy
        torch.cuda.empty_cache()
    print(f"   ãƒ¢ãƒ‡ãƒ«ã‚’GPUã«ç§»å‹•ã—ã¾ã—ãŸ")

print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†\n")


def start_session(request: gr.Request):
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹æ™‚ã«ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
    session_hash = request.session_hash if request else "default"
    session_dir = TEMP_DIR / session_hash
    session_dir.mkdir(parents=True, exist_ok=True)
    print(f"Session started: {session_hash}")
    return session_dir.as_posix()


def end_session(request: gr.Request):
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
    session_hash = request.session_hash if request else "default"
    session_dir = TEMP_DIR / session_hash
    
    if session_dir.exists():
        shutil.rmtree(session_dir)
    
    print(f"Session ended: {session_hash}")


def extract_audio_from_video(video_path: str, output_path: str) -> bool:
    """å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰éŸ³å£°ã‚’æŠ½å‡ºï¼ˆ16kHz mono WAVï¼‰"""
    try:
        cmd = [
            "ffmpeg", "-y",
            "-i", video_path,
            "-vn",
            "-acodec", "pcm_s16le",
            "-ar", "16000",
            "-ac", "1",
            output_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        return result.returncode == 0
    except Exception as e:
        print(f"FFmpeg error: {e}")
        return False


def get_audio_segment(audio_path, start_second, end_second):
    """éŸ³å£°ã®ä¸€éƒ¨ã‚’åˆ‡ã‚Šå‡ºã—"""
    if not audio_path or not Path(audio_path).exists():
        return None
    try:
        start_ms = int(start_second * 1000)
        end_ms = int(end_second * 1000)
        start_ms = max(0, start_ms)
        if end_ms <= start_ms:
            end_ms = start_ms + 100

        audio = AudioSegment.from_file(audio_path)
        clipped_audio = audio[start_ms:end_ms]
        
        samples = np.array(clipped_audio.get_array_of_samples())
        if clipped_audio.channels == 2:
            samples = samples.reshape((-1, 2)).mean(axis=1).astype(samples.dtype)
        
        frame_rate = clipped_audio.frame_rate
        if frame_rate <= 0:
            frame_rate = audio.frame_rate
        
        if samples.size == 0:
            return None
        
        return (frame_rate, samples)
    except Exception as e:
        print(f"Error clipping audio: {e}")
        return None


def format_srt_time(seconds: float) -> str:
    """ç§’æ•°ã‚’SRTå½¢å¼ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«å¤‰æ›"""
    sanitized_total_seconds = max(0.0, seconds)
    delta = datetime.timedelta(seconds=sanitized_total_seconds)
    total_int_seconds = int(delta.total_seconds())
    
    hours = total_int_seconds // 3600
    remainder_seconds_after_hours = total_int_seconds % 3600
    minutes = remainder_seconds_after_hours // 60
    seconds_part = remainder_seconds_after_hours % 60
    milliseconds = delta.microseconds // 1000
    
    return f"{hours:02d}:{minutes:02d}:{seconds_part:02d},{milliseconds:03d}"


def generate_srt_content(segment_timestamps: list) -> str:
    """SRTå½¢å¼ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ"""
    srt_content = []
    for i, ts in enumerate(segment_timestamps):
        start_time = format_srt_time(ts['start'])
        end_time = format_srt_time(ts['end'])
        text = ts['segment']
        srt_content.append(str(i + 1))
        srt_content.append(f"{start_time} --> {end_time}")
        srt_content.append(text)
        srt_content.append("")
    return "\n".join(srt_content)


def transcribe_audio(audio_input, session_dir):
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãèµ·ã“ã—"""
    # gr.Fileã‹ã‚‰ã®ãƒ‘ã‚¹å–å¾—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¾ãŸã¯ãƒ‘ã‚¹æ–‡å­—åˆ—ã«å¯¾å¿œï¼‰
    if audio_input is None:
        print("ã‚¨ãƒ©ãƒ¼: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return (
            [], [], None,
            gr.update(visible=False),
            gr.update(visible=False),
            gr.update(visible=False),
            ""
        )
    
    # gr.Fileã®å ´åˆã¯ãƒ‘ã‚¹ã‚’å–å¾—ã€gr.Audioã®å ´åˆã¯ãã®ã¾ã¾
    if hasattr(audio_input, 'name'):
        audio_path = audio_input.name
    elif isinstance(audio_input, str):
        audio_path = audio_input
    else:
        audio_path = str(audio_input)
    
    vis_data = [["N/A", "N/A", "å‡¦ç†å¤±æ•—"]]
    raw_times_data = [[0.0, 0.0]]
    processed_audio_path = None
    csv_file_path = None
    srt_file_path = None
    txt_file_path = None
    full_text = ""
    
    original_path_name = Path(audio_path).name
    audio_name = Path(audio_path).stem
    
    # ãƒœã‚¿ãƒ³ã®åˆæœŸçŠ¶æ…‹ï¼ˆgr.update()ã‚’ä½¿ç”¨ï¼‰
    csv_button = gr.update(visible=False)
    srt_button = gr.update(visible=False)
    txt_button = gr.update(visible=False)
    
    long_audio_settings_applied = False
    
    try:
        # å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯éŸ³å£°ã‚’æŠ½å‡º
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".webm", ".flv", ".wmv"}
        if Path(audio_path).suffix.lower() in video_extensions:
            print(f"ğŸ¬ å‹•ç”»ã‹ã‚‰éŸ³å£°ã‚’æŠ½å‡ºä¸­: {original_path_name}")
            temp_audio_path = Path(session_dir) / f"{audio_name}_extracted.wav"
            if not extract_audio_from_video(audio_path, str(temp_audio_path)):
                print("ã‚¨ãƒ©ãƒ¼: å‹•ç”»ã‹ã‚‰ã®éŸ³å£°æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
                return vis_data, raw_times_data, audio_path, gr.update(visible=False), gr.update(visible=False), gr.update(visible=False), ""
            audio_path = str(temp_audio_path)
            original_path_name = temp_audio_path.name
        
        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        print(f"ğŸœ éŸ³å£°ã‚’èª­ã¿è¾¼ã¿ä¸­: {original_path_name}")
        audio = AudioSegment.from_file(audio_path)
        duration_sec = audio.duration_seconds
        
        # 16kHz ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
        resampled = False
        mono = False
        
        target_sr = 16000
        if audio.frame_rate != target_sr:
            audio = audio.set_frame_rate(target_sr)
            resampled = True
        
        if audio.channels == 2:
            audio = audio.set_channels(1)
            mono = True
        elif audio.channels > 2:
            print(f"ã‚¨ãƒ©ãƒ¼: éŸ³å£°ãŒ{audio.channels}ãƒãƒ£ãƒ³ãƒãƒ«ã§ã™")
            return vis_data, raw_times_data, audio_path, gr.update(visible=False), gr.update(visible=False), gr.update(visible=False), ""
        
        if resampled or mono:
            processed_audio_path = Path(session_dir) / f"{audio_name}_processed.wav"
            audio.export(processed_audio_path, format="wav")
            transcribe_path = processed_audio_path.as_posix()
        else:
            transcribe_path = audio_path
        
        print(f"ğŸ“ æ›¸ãèµ·ã“ã—ä¸­... ({duration_sec:.1f}ç§’)")
        
        # é•·ã„éŸ³å£°ã®å ´åˆã¯æœ€é©åŒ–è¨­å®šã‚’é©ç”¨
        if duration_sec > 480:  # 8åˆ†ä»¥ä¸Š
            try:
                print("âš¡ é•·ã„éŸ³å£°ã®ãŸã‚æœ€é©åŒ–è¨­å®šã‚’é©ç”¨ä¸­...")
                model.change_attention_model("rel_pos_local_attn", [256, 256])
                model.change_subsampling_conv_chunking_factor(1)
                long_audio_settings_applied = True
            except Exception as e:
                print(f"Warning: Failed to apply long audio settings: {e}")
        
        # æ¨è«–
        output = model.transcribe([transcribe_path], timestamps=True)
        
        if not output or not isinstance(output, list) or not output[0]:
            print("ã‚¨ãƒ©ãƒ¼: æ›¸ãèµ·ã“ã—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return vis_data, raw_times_data, audio_path, gr.update(visible=False), gr.update(visible=False), gr.update(visible=False), ""
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—
        if hasattr(output[0], 'timestamp') and output[0].timestamp and 'segment' in output[0].timestamp:
            segment_timestamps = output[0].timestamp['segment']
        else:
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒãªã„å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‚’1ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¨ã—ã¦æ‰±ã†
            text = output[0].text if hasattr(output[0], 'text') else str(output[0])
            segment_timestamps = [{'start': 0.0, 'end': duration_sec, 'segment': text}]
        
        print(f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {len(segment_timestamps)}")
        
        # ãƒ‡ãƒ¼ã‚¿æ•´å½¢
        csv_headers = ["é–‹å§‹ (ç§’)", "çµ‚äº† (ç§’)", "ãƒ†ã‚­ã‚¹ãƒˆ"]
        vis_data = [[f"{ts['start']:.2f}", f"{ts['end']:.2f}", ts['segment']] for ts in segment_timestamps]
        raw_times_data = [[ts['start'], ts['end']] for ts in segment_timestamps]
        
        # ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
        full_text = "".join([ts['segment'] for ts in segment_timestamps])
        print(f"ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(full_text)} æ–‡å­—")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        try:
            csv_file_path = Path(session_dir) / f"{audio_name}_transcript.csv"
            with open(csv_file_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(csv_headers)
                writer.writerows(vis_data)
            csv_button = gr.update(value=str(csv_file_path), visible=True)
        except Exception as e:
            print(f"CSV error: {e}")
        
        # SRTãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        try:
            srt_content = generate_srt_content(segment_timestamps)
            srt_file_path = Path(session_dir) / f"{audio_name}_transcript.srt"
            with open(srt_file_path, 'w', encoding='utf-8') as f:
                f.write(srt_content)
            srt_button = gr.update(value=str(srt_file_path), visible=True)
        except Exception as e:
            print(f"SRT error: {e}")
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        try:
            txt_file_path = Path(session_dir) / f"{audio_name}_transcript.txt"
            with open(txt_file_path, 'w', encoding='utf-8') as f:
                f.write(full_text)
            txt_button = gr.update(value=str(txt_file_path), visible=True)
        except Exception as e:
            print(f"TXT error: {e}")
        
        print("âœ… æ›¸ãèµ·ã“ã—å®Œäº†ï¼ çµæœã‚’è¿”ã—ã¾ã™...")
        
        # DataFrameã®è¡¨ç¤ºã‚’æœ€å¤§500è¡Œã«åˆ¶é™ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ã®è² è·è»½æ¸›ï¼‰
        MAX_DISPLAY_ROWS = 500
        if len(vis_data) > MAX_DISPLAY_ROWS:
            print(f"âš ï¸ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°ãŒå¤šã„ãŸã‚è¡¨ç¤ºã‚’{MAX_DISPLAY_ROWS}è¡Œã«åˆ¶é™ã—ã¾ã™ï¼ˆå…¨{len(vis_data)}è¡Œã¯CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ï¼‰")
            display_vis_data = vis_data[:MAX_DISPLAY_ROWS]
            display_raw_times = raw_times_data[:MAX_DISPLAY_ROWS]
        else:
            display_vis_data = vis_data
            display_raw_times = raw_times_data
        
        return display_vis_data, display_raw_times, audio_path, csv_button, srt_button, txt_button, full_text
    
    except torch.cuda.OutOfMemoryError:
        error_msg = 'GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã§ã™ã€‚ã‚ˆã‚ŠçŸ­ã„éŸ³å£°ã§è©¦ã—ã¦ãã ã•ã„ã€‚'
        print(f"OOM Error: {error_msg}")
        return [["OOM", "OOM", error_msg]], [[0.0, 0.0]], audio_path, gr.update(visible=False), gr.update(visible=False), gr.update(visible=False), ""
    
    except Exception as e:
        error_msg = f"ã‚¨ãƒ©ãƒ¼: {e}"
        print(f"Transcription error: {e}")
        return [["Error", "Error", error_msg]], [[0.0, 0.0]], audio_path, gr.update(visible=False), gr.update(visible=False), gr.update(visible=False), ""
    
    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        try:
            if long_audio_settings_applied:
                model.change_attention_model("rel_pos")
                model.change_subsampling_conv_chunking_factor(-1)
            
            # GPUãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ãŒã€ãƒ¢ãƒ‡ãƒ«ã¯GPUã«ä¿æŒ
            gc.collect()
            if device == 'cuda':
                torch.cuda.empty_cache()
        except Exception as e:
            print(f"Cleanup error: {e}")
        
        # å‡¦ç†æ¸ˆã¿éŸ³å£°ã‚’å‰Šé™¤
        if processed_audio_path and os.path.exists(processed_audio_path):
            try:
                os.remove(processed_audio_path)
            except Exception:
                pass


def play_segment(evt: gr.SelectData, raw_ts_list, current_audio_path):
    """ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡Œã‚’ã‚¯ãƒªãƒƒã‚¯ã—ãŸã¨ãã«ãã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å†ç”Ÿ"""
    if not isinstance(raw_ts_list, list) or not current_audio_path:
        return gr.Audio(value=None, label="é¸æŠã—ãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
    
    selected_index = evt.index[0]
    
    if selected_index < 0 or selected_index >= len(raw_ts_list):
        return gr.Audio(value=None, label="é¸æŠã—ãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
    
    if not isinstance(raw_ts_list[selected_index], (list, tuple)) or len(raw_ts_list[selected_index]) != 2:
        return gr.Audio(value=None, label="é¸æŠã—ãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
    
    start_time_s, end_time_s = raw_ts_list[selected_index]
    segment_data = get_audio_segment(current_audio_path, start_time_s, end_time_s)
    
    if segment_data:
        return gr.Audio(
            value=segment_data, 
            autoplay=True, 
            label=f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ: {start_time_s:.2f}ç§’ - {end_time_s:.2f}ç§’",
            interactive=False
        )
    else:
        return gr.Audio(value=None, label="é¸æŠã—ãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")


# ========================================
# Gradio UI
# ========================================
css = """
.main-title {
    text-align: center;
    margin-bottom: 1rem;
}
.info-box {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    padding: 1rem;
    border-radius: 10px;
    color: white;
    margin-bottom: 1rem;
}
"""

with gr.Blocks(
    title="ğŸ¦œ Parakeet æ—¥æœ¬èªéŸ³å£°æ›¸ãèµ·ã“ã—",
    css=css,
    theme=gr.themes.Soft()
) as demo:
    
    gr.Markdown("""
    # ğŸ¦œ Parakeet TDT-CTC 0.6B-ja
    ## æ—¥æœ¬èªéŸ³å£°æ›¸ãèµ·ã“ã—ãƒ„ãƒ¼ãƒ«
    
    å‹•ç”»/éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã«æ›¸ãèµ·ã“ã—ã¾ã™ã€‚
    
    **å¯¾å¿œå½¢å¼**: MP4, MKV, AVI, MOV, WebM, WAV, MP3, FLAC, OGG, M4A
    """)
    
    # çŠ¶æ…‹ç®¡ç†
    current_audio_path_state = gr.State(None)
    raw_timestamps_list_state = gr.State([])
    session_dir = gr.State()
    
    demo.load(start_session, outputs=[session_dir])
    
    with gr.Tabs():
        with gr.TabItem("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"):
            file_input = gr.File(
                label="éŸ³å£°/å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
                file_types=[".mp4", ".mkv", ".avi", ".mov", ".webm", ".flv", ".wmv", 
                           ".wav", ".mp3", ".flac", ".ogg", ".m4a", ".aac"]
            )
            file_transcribe_btn = gr.Button("ğŸ™ï¸ æ›¸ãèµ·ã“ã—é–‹å§‹", variant="primary", size="lg")
        
        with gr.TabItem("ğŸ¤ ãƒã‚¤ã‚¯éŒ²éŸ³"):
            mic_input = gr.Audio(
                sources=["microphone"], 
                type="filepath", 
                label="ãƒã‚¤ã‚¯ã§éŒ²éŸ³"
            )
            mic_transcribe_btn = gr.Button("ğŸ™ï¸ æ›¸ãèµ·ã“ã—é–‹å§‹", variant="primary", size="lg")
    
    gr.Markdown("---")
    gr.Markdown("### ğŸ“„ æ›¸ãèµ·ã“ã—çµæœ")
    
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
    with gr.Row():
        download_btn_txt = gr.DownloadButton(label="ğŸ“¥ ãƒ†ã‚­ã‚¹ãƒˆ", visible=False)
        download_btn_srt = gr.DownloadButton(label="ğŸ“¥ SRTå­—å¹•", visible=False)
        download_btn_csv = gr.DownloadButton(label="ğŸ“¥ CSV", visible=False)
    
    # ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
    full_text_output = gr.Textbox(
        label="æ›¸ãèµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚³ãƒ”ãƒ¼å¯èƒ½ï¼‰",
        lines=5,
        max_lines=10
    )
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ†ãƒ¼ãƒ–ãƒ«
    gr.Markdown("### â±ï¸ ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆã‚¯ãƒªãƒƒã‚¯ã§å†ç”Ÿï¼‰")
    
    vis_timestamps_df = gr.DataFrame(
        headers=["é–‹å§‹ (ç§’)", "çµ‚äº† (ç§’)", "ãƒ†ã‚­ã‚¹ãƒˆ"],
        datatype=["str", "str", "str"],
        wrap=True,
        label="ã‚»ã‚°ãƒ¡ãƒ³ãƒˆä¸€è¦§"
    )
    
    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå†ç”Ÿãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼
    selected_segment_player = gr.Audio(label="é¸æŠã—ãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆ", interactive=False)
    
    # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©
    file_transcribe_btn.click(
        fn=transcribe_audio,
        inputs=[file_input, session_dir],
        outputs=[
            vis_timestamps_df, 
            raw_timestamps_list_state, 
            current_audio_path_state, 
            download_btn_csv, 
            download_btn_srt, 
            download_btn_txt,
            full_text_output
        ]
    )
    
    mic_transcribe_btn.click(
        fn=transcribe_audio,
        inputs=[mic_input, session_dir],
        outputs=[
            vis_timestamps_df, 
            raw_timestamps_list_state, 
            current_audio_path_state, 
            download_btn_csv, 
            download_btn_srt, 
            download_btn_txt,
            full_text_output
        ]
    )
    
    vis_timestamps_df.select(
        fn=play_segment,
        inputs=[raw_timestamps_list_state, current_audio_path_state],
        outputs=[selected_segment_player]
    )
    
    demo.unload(end_session)
    
    gr.Markdown("""
    ---
    ### ğŸ“‹ ä½¿ã„æ–¹
    1. **ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰** ã¾ãŸã¯ **ãƒã‚¤ã‚¯éŒ²éŸ³** ã‚¿ãƒ–ã‚’é¸æŠ
    2. éŸ³å£°/å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ã¾ãŸã¯éŒ²éŸ³
    3. ã€Œæ›¸ãèµ·ã“ã—é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
    4. çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆã€SRTå­—å¹•ã€ã¾ãŸã¯CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    5. ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡Œã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€ãã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å†ç”Ÿã§ãã¾ã™
    
    ### âš™ï¸ æŠ€è¡“æƒ…å ±
    - **ãƒ¢ãƒ‡ãƒ«**: NVIDIA Parakeet TDT-CTC 0.6B-ja
    - **ãƒ‡ãƒã‚¤ã‚¹**: """ + device + """
    - **å¯¾å¿œ**: æ—¥æœ¬èªéŸ³å£°èªè­˜ã€å¥èª­ç‚¹è‡ªå‹•æŒ¿å…¥
    """)


if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¦œ Parakeet æ—¥æœ¬èªéŸ³å£°æ›¸ãèµ·ã“ã— Web UI")
    print("=" * 60)
    print(f"\nğŸ“‚ ãƒ¢ãƒ‡ãƒ«: {MODEL_PATH}")
    print(f"ğŸ’» ãƒ‡ãƒã‚¤ã‚¹: {device}")
    print("\nğŸŒ ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:7860 ã‚’é–‹ã„ã¦ãã ã•ã„\n")
    
    demo.queue()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )
