#!/usr/bin/env python3
"""
ğŸ¦œ Speech-to-Text Batch Processor (æ—¥æœ¬èª/English)
è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ‹¬æ›¸ãèµ·ã“ã—ãƒ„ãƒ¼ãƒ«ï¼ˆWSL2å¯¾å¿œç‰ˆï¼‰

â€» WSL2ã§ã¯CTCãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨ï¼ˆCUDA Graphsã‚’å›é¿ï¼‰
   è©³ç´°ã¯ WSL2_CUDA_ERROR_README.md ã‚’å‚ç…§
"""

from nemo.collections.asr.models import ASRModel
import torch
import gradio as gr
import gc
from pathlib import Path
from pydub import AudioSegment
import subprocess
import datetime
import zipfile
import tempfile

# ========================================
# è¨­å®š
# ========================================
SCRIPT_DIR = Path(__file__).parent.resolve()
TEMP_DIR = SCRIPT_DIR / "temp"
TEMP_DIR.mkdir(exist_ok=True)

# åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«
MODELS = {
    "æ—¥æœ¬èª (Parakeet TDT-CTC 0.6B-ja)": {
        "path": SCRIPT_DIR / "parakeet-tdt_ctc-0.6b-ja.nemo",
        "lang": "ja",
    },
    "English (Nemotron Speech 0.6B-en)": {
        "path": SCRIPT_DIR / "nemotron-speech-streaming-en-0.6b.nemo",
        "lang": "en",
    },
}

# GPUã‚’ä½¿ç”¨
device = "cuda" if torch.cuda.is_available() else "cpu"

# ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼‰
current_model = None
current_model_name = None


def load_model(model_name: str):
    """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆã¾ãŸã¯åˆ‡ã‚Šæ›¿ãˆï¼‰"""
    global current_model, current_model_name

    if current_model_name == model_name and current_model is not None:
        return True

    model_info = MODELS.get(model_name)
    if not model_info or not model_info["path"].exists():
        print(
            f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_info['path'] if model_info else model_name}"
        )
        return False

    print(f"\nğŸ¦œ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {model_name}")
    print(f"   ãƒ‘ã‚¹: {model_info['path']}")
    print(f"   ãƒ‡ãƒã‚¤ã‚¹: {device}")

    # å¤ã„ãƒ¢ãƒ‡ãƒ«ã‚’è§£æ”¾
    if current_model is not None:
        del current_model
        gc.collect()
        if device == "cuda":
            torch.cuda.empty_cache()

    # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
    current_model = ASRModel.restore_from(str(model_info["path"]))
    current_model.eval()

    if device == "cuda":
        current_model = current_model.cuda()
        current_model.cur_decoder = "ctc"
        print("   ãƒ¢ãƒ‡ãƒ«ã‚’GPUã«ç§»å‹•ã—ã¾ã—ãŸ (CTCãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ä½¿ç”¨)")
    else:
        print("   CPUãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œä¸­")

    current_model_name = model_name
    print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†\n")
    return True


def extract_audio_from_video(video_path: str, output_path: str) -> bool:
    """å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰éŸ³å£°ã‚’æŠ½å‡º"""
    try:
        cmd = [
            "ffmpeg",
            "-y",
            "-i",
            video_path,
            "-vn",
            "-acodec",
            "pcm_s16le",
            "-ar",
            "16000",
            "-ac",
            "1",
            output_path,
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        return result.returncode == 0
    except Exception as e:
        print(f"FFmpeg error: {e}")
        return False


def format_srt_time(seconds: float) -> str:
    """ç§’æ•°ã‚’SRTå½¢å¼ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«å¤‰æ›"""
    delta = datetime.timedelta(seconds=max(0.0, seconds))
    total_int_seconds = int(delta.total_seconds())
    hours = total_int_seconds // 3600
    minutes = (total_int_seconds % 3600) // 60
    seconds_part = total_int_seconds % 60
    milliseconds = delta.microseconds // 1000
    return f"{hours:02d}:{minutes:02d}:{seconds_part:02d},{milliseconds:03d}"


def transcribe_single_file(audio_path: str, audio_name: str) -> tuple:
    """
    å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãèµ·ã“ã—

    Returns:
        (segments, error_message)
    """
    MAX_CHUNK_SEC = 300  # 5åˆ†

    try:
        # å‹•ç”»ã®å ´åˆã¯éŸ³å£°æŠ½å‡º
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".webm", ".flv", ".wmv"}
        if Path(audio_path).suffix.lower() in video_extensions:
            print(f"   ğŸ¬ å‹•ç”»ã‹ã‚‰éŸ³å£°ã‚’æŠ½å‡ºä¸­...")
            temp_audio = TEMP_DIR / f"{audio_name}_extracted.wav"
            if not extract_audio_from_video(audio_path, str(temp_audio)):
                return None, "å‹•ç”»ã‹ã‚‰ã®éŸ³å£°æŠ½å‡ºã«å¤±æ•—"
            audio_path = str(temp_audio)

        # éŸ³å£°èª­ã¿è¾¼ã¿ãƒ»å¤‰æ›
        audio = AudioSegment.from_file(audio_path)
        duration_sec = audio.duration_seconds

        if audio.frame_rate != 16000:
            audio = audio.set_frame_rate(16000)
        if audio.channels != 1:
            audio = audio.set_channels(1)

        processed_path = TEMP_DIR / f"{audio_name}_processed.wav"
        audio.export(processed_path, format="wav")

        # æ›¸ãèµ·ã“ã—
        all_segments = []

        if duration_sec > MAX_CHUNK_SEC:
            # é•·ã„éŸ³å£°: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å‡¦ç†
            chunk_start = 0
            chunk_idx = 0

            while chunk_start < duration_sec:
                chunk_end = min(chunk_start + MAX_CHUNK_SEC, duration_sec)

                chunk_audio = audio[int(chunk_start * 1000) : int(chunk_end * 1000)]
                chunk_path = TEMP_DIR / f"{audio_name}_chunk_{chunk_idx}.wav"
                chunk_audio.export(chunk_path, format="wav")

                try:
                    output = current_model.transcribe(
                        [str(chunk_path)], timestamps=True
                    )

                    if output and output[0]:
                        if (
                            hasattr(output[0], "timestamp")
                            and output[0].timestamp
                            and "segment" in output[0].timestamp
                        ):
                            chunk_segments = output[0].timestamp["segment"]
                            for seg in chunk_segments:
                                seg["start"] += chunk_start
                                seg["end"] += chunk_start
                            all_segments.extend(chunk_segments)
                        else:
                            text = (
                                output[0].text
                                if hasattr(output[0], "text")
                                else str(output[0])
                            )
                            all_segments.append(
                                {
                                    "start": chunk_start,
                                    "end": chunk_end,
                                    "segment": text,
                                }
                            )
                finally:
                    if chunk_path.exists():
                        chunk_path.unlink()
                    gc.collect()
                    if device == "cuda":
                        torch.cuda.empty_cache()

                chunk_start = chunk_end
                chunk_idx += 1
        else:
            # çŸ­ã„éŸ³å£°: ãã®ã¾ã¾å‡¦ç†
            output = current_model.transcribe([str(processed_path)], timestamps=True)

            if output and output[0]:
                if (
                    hasattr(output[0], "timestamp")
                    and output[0].timestamp
                    and "segment" in output[0].timestamp
                ):
                    all_segments = output[0].timestamp["segment"]
                else:
                    text = (
                        output[0].text if hasattr(output[0], "text") else str(output[0])
                    )
                    all_segments = [
                        {"start": 0.0, "end": duration_sec, "segment": text}
                    ]

        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if processed_path.exists():
            processed_path.unlink()

        return all_segments, None

    except Exception as e:
        return None, str(e)


def save_transcript_files(segments: list, audio_name: str, output_dir: Path) -> tuple:
    """æ›¸ãèµ·ã“ã—çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    full_text = "".join([s["segment"] for s in segments])

    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
    txt_path = output_dir / f"{audio_name}_transcript.txt"
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write(full_text)

    # SRTãƒ•ã‚¡ã‚¤ãƒ«
    srt_path = output_dir / f"{audio_name}_transcript.srt"
    with open(srt_path, "w", encoding="utf-8") as f:
        for i, s in enumerate(segments):
            f.write(f"{i+1}\n")
            f.write(f"{format_srt_time(s['start'])} --> {format_srt_time(s['end'])}\n")
            f.write(f"{s['segment']}\n\n")

    # CSVãƒ•ã‚¡ã‚¤ãƒ«
    csv_path = output_dir / f"{audio_name}_transcript.csv"
    with open(csv_path, "w", encoding="utf-8") as f:
        f.write("Start,End,Text\n")
        for s in segments:
            f.write(f"{s['start']:.2f},{s['end']:.2f},\"{s['segment']}\"\n")

    return txt_path, srt_path, csv_path


def batch_transcribe(files, model_name, progress=gr.Progress()):
    """
    è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬æ›¸ãèµ·ã“ã—
    """
    if not files:
        return "âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„", None, ""

    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    if not load_model(model_name):
        return f"âŒ ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {model_name}", None, ""

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    batch_id = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = TEMP_DIR / f"batch_{batch_id}"
    output_dir.mkdir(exist_ok=True)

    results = []
    errors = []
    total_files = len(files)

    print(f"\n{'='*60}")
    print(f"ğŸ¦œ ãƒãƒƒãƒå‡¦ç†é–‹å§‹: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«")
    print(f"   ãƒ¢ãƒ‡ãƒ«: {model_name}")
    print(f"{'='*60}\n")

    for idx, file in enumerate(files):
        file_path = file.name if hasattr(file, "name") else str(file)
        file_name = Path(file_path).stem

        progress(
            (idx + 1) / total_files, f"å‡¦ç†ä¸­: {file_name} ({idx + 1}/{total_files})"
        )
        print(f"ğŸ“ [{idx + 1}/{total_files}] {file_name}")

        segments, error = transcribe_single_file(file_path, file_name)

        if error:
            errors.append(f"{file_name}: {error}")
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {error}")
        elif segments:
            txt_path, srt_path, csv_path = save_transcript_files(
                segments, file_name, output_dir
            )
            full_text = "".join([s["segment"] for s in segments])
            results.append(
                {
                    "file": file_name,
                    "text": full_text,
                    "segments": len(segments),
                    "chars": len(full_text),
                }
            )
            print(f"   âœ… å®Œäº†: {len(segments)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ, {len(full_text)}æ–‡å­—")
        else:
            errors.append(f"{file_name}: æ›¸ãèµ·ã“ã—çµæœãŒç©ºã§ã™")
            print(f"   âš ï¸ çµæœãŒç©º")

    # ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    zip_path = TEMP_DIR / f"batch_{batch_id}.zip"
    with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
        for file_path in output_dir.glob("*"):
            zf.write(file_path, file_path.name)

    # ã‚µãƒãƒªãƒ¼ä½œæˆ
    summary_lines = [
        f"# ãƒãƒƒãƒå‡¦ç†çµæœ",
        f"",
        f"- **å‡¦ç†æ—¥æ™‚**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"- **ãƒ¢ãƒ‡ãƒ«**: {model_name}",
        f"- **å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {len(results)}/{total_files}",
        f"",
    ]

    if results:
        summary_lines.append("## âœ… æˆåŠŸ")
        for r in results:
            summary_lines.append(
                f"- **{r['file']}**: {r['segments']}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ, {r['chars']}æ–‡å­—"
            )

    if errors:
        summary_lines.append("")
        summary_lines.append("## âŒ ã‚¨ãƒ©ãƒ¼")
        for e in errors:
            summary_lines.append(f"- {e}")

    summary = "\n".join(summary_lines)

    # è©³ç´°çµæœï¼ˆå„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆï¼‰
    detail_lines = []
    for r in results:
        detail_lines.append(f"{'='*60}")
        detail_lines.append(f"ğŸ“„ {r['file']}")
        detail_lines.append(f"{'='*60}")
        detail_lines.append(r["text"])
        detail_lines.append("")

    detail_text = "\n".join(detail_lines)

    print(f"\n{'='*60}")
    print(f"ğŸ‰ ãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(results)}/{total_files}ãƒ•ã‚¡ã‚¤ãƒ«æˆåŠŸ")
    print(f"{'='*60}\n")

    return summary, str(zip_path), detail_text


# ========================================
# Gradio UI
# ========================================
with gr.Blocks(title="ğŸ¦œ Batch Speech-to-Text") as demo:

    gr.Markdown(
        """
    # ğŸ¦œ Batch Speech-to-Text
    ## è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ä¸€æ‹¬æ›¸ãèµ·ã“ã—ï¼ˆæ—¥æœ¬èª/Englishï¼‰
    
    è¤‡æ•°ã®å‹•ç”»/éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬ã§æ›¸ãèµ·ã“ã—ã¾ã™ã€‚
    """
    )

    with gr.Row():
        model_dropdown = gr.Dropdown(
            choices=list(MODELS.keys()),
            value=list(MODELS.keys())[0],
            label="ğŸ”„ è¨€èª/ãƒ¢ãƒ‡ãƒ«é¸æŠ",
            info="æ›¸ãèµ·ã“ã—ã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
        )

    file_input = gr.File(
        label="ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰",
        file_count="multiple",
        file_types=[
            ".mp4",
            ".mkv",
            ".avi",
            ".mov",
            ".webm",
            ".wav",
            ".mp3",
            ".flac",
            ".m4a",
        ],
    )

    transcribe_btn = gr.Button("ğŸš€ ä¸€æ‹¬æ›¸ãèµ·ã“ã—é–‹å§‹", variant="primary", size="lg")

    gr.Markdown("---")
    gr.Markdown("### ğŸ“Š å‡¦ç†çµæœ")

    summary_output = gr.Markdown(label="ã‚µãƒãƒªãƒ¼")

    zip_output = gr.File(label="ğŸ“¦ çµæœãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (ZIP)")

    with gr.Accordion("ğŸ“„ è©³ç´°çµæœï¼ˆå„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆï¼‰", open=False):
        detail_output = gr.Textbox(label="å…¨ãƒ†ã‚­ã‚¹ãƒˆ", lines=20)

    transcribe_btn.click(
        fn=batch_transcribe,
        inputs=[file_input, model_dropdown],
        outputs=[summary_output, zip_output, detail_output],
    )


if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¦œ Batch Speech-to-Text (æ—¥æœ¬èª/English)")
    print("=" * 60)
    print("\nğŸŒ ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:7862 ã‚’é–‹ã„ã¦ãã ã•ã„\n")

    demo.launch(server_name="0.0.0.0", server_port=7862)
