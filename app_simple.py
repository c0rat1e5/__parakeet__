#!/usr/bin/env python3
"""
ğŸ¦œ Parakeet TDT-CTC 0.6B-ja Web UI
æ—¥æœ¬èªéŸ³å£°æ›¸ãèµ·ã“ã—ãƒ„ãƒ¼ãƒ«ï¼ˆWSL2å¯¾å¿œç‰ˆï¼‰

â€» WSL2ã§ã¯CTCãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨ï¼ˆCUDA Graphsã‚’å›é¿ï¼‰
   è©³ç´°ã¯ WSL2_CUDA_ERROR_README.md ã‚’å‚ç…§
"""

from nemo.collections.asr.models import ASRModel
import torch
import gradio as gr
import gc
from pathlib import Path
from pydub import AudioSegment
import subprocess
import datetime

# ========================================
# è¨­å®š
# ========================================
SCRIPT_DIR = Path(__file__).parent.resolve()
MODEL_PATH = SCRIPT_DIR / "parakeet-tdt_ctc-0.6b-ja.nemo"
TEMP_DIR = SCRIPT_DIR / "temp"
TEMP_DIR.mkdir(exist_ok=True)

# GPUã‚’ä½¿ç”¨
device = "cuda" if torch.cuda.is_available() else "cpu"

# ãƒ¢ãƒ‡ãƒ«ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«èª­ã¿è¾¼ã¿
print(f"ğŸ¦œ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {MODEL_PATH}")
print(f"   ãƒ‡ãƒã‚¤ã‚¹: {device}")

if not MODEL_PATH.exists():
    raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {MODEL_PATH}")

model = ASRModel.restore_from(str(MODEL_PATH))
model.eval()

# GPUã«ç§»å‹•
if device == "cuda":
    model = model.cuda()
    # WSL2äº’æ›æ€§ã®ãŸã‚CTCãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨ï¼ˆCUDA Graphsã‚’å›é¿ï¼‰
    model.cur_decoder = "ctc"
    print("   ãƒ¢ãƒ‡ãƒ«ã‚’GPUã«ç§»å‹•ã—ã¾ã—ãŸ (CTCãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ä½¿ç”¨)")
else:
    print("   CPUãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œä¸­")

print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†\n")


def extract_audio_from_video(video_path: str, output_path: str) -> bool:
    """å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰éŸ³å£°ã‚’æŠ½å‡º"""
    try:
        cmd = [
            "ffmpeg", "-y", "-i", video_path,
            "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1",
            output_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        return result.returncode == 0
    except Exception as e:
        print(f"FFmpeg error: {e}")
        return False


def format_srt_time(seconds: float) -> str:
    """ç§’æ•°ã‚’SRTå½¢å¼ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«å¤‰æ›"""
    delta = datetime.timedelta(seconds=max(0.0, seconds))
    total_int_seconds = int(delta.total_seconds())
    hours = total_int_seconds // 3600
    minutes = (total_int_seconds % 3600) // 60
    seconds_part = total_int_seconds % 60
    milliseconds = delta.microseconds // 1000
    return f"{hours:02d}:{minutes:02d}:{seconds_part:02d},{milliseconds:03d}"


def transcribe_short_audio(audio_path: str, duration_sec: float) -> list:
    """
    çŸ­ã„éŸ³å£°ï¼ˆ5åˆ†ä»¥ä¸‹ï¼‰ã‚’ãã®ã¾ã¾æ›¸ãèµ·ã“ã—
    
    Args:
        audio_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        duration_sec: éŸ³å£°ã®é•·ã•ï¼ˆç§’ï¼‰
    
    Returns:
        ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ [{'start': float, 'end': float, 'segment': str}, ...]
    """
    output = model.transcribe([audio_path], timestamps=True)
    
    if not output or not output[0]:
        return []
    
    if hasattr(output[0], 'timestamp') and output[0].timestamp and 'segment' in output[0].timestamp:
        return output[0].timestamp['segment']
    else:
        text = output[0].text if hasattr(output[0], 'text') else str(output[0])
        return [{'start': 0.0, 'end': duration_sec, 'segment': text}]


def transcribe_long_audio(audio: AudioSegment, audio_name: str, duration_sec: float) -> list:
    """
    é•·ã„éŸ³å£°ã‚’5åˆ†ã”ã¨ã«åˆ†å‰²ã—ã¦æ›¸ãèµ·ã“ã—
    
    Args:
        audio: AudioSegmentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        audio_name: ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰
        duration_sec: éŸ³å£°ã®é•·ã•ï¼ˆç§’ï¼‰
    
    Returns:
        ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ [{'start': float, 'end': float, 'segment': str}, ...]
    """
    MAX_CHUNK_SEC = 300  # 5åˆ†
    
    print(f"âš¡ é•·ã„éŸ³å£°ã®ãŸã‚{MAX_CHUNK_SEC}ç§’ã”ã¨ã«åˆ†å‰²å‡¦ç†...")
    all_segments = []
    chunk_start = 0
    chunk_idx = 0
    
    while chunk_start < duration_sec:
        chunk_end = min(chunk_start + MAX_CHUNK_SEC, duration_sec)
        print(f"   ãƒãƒ£ãƒ³ã‚¯ {chunk_idx + 1}: {chunk_start:.0f}ç§’ - {chunk_end:.0f}ç§’")
        
        # ãƒãƒ£ãƒ³ã‚¯ã‚’åˆ‡ã‚Šå‡ºã—
        chunk_audio = audio[int(chunk_start * 1000):int(chunk_end * 1000)]
        chunk_path = TEMP_DIR / f"{audio_name}_chunk_{chunk_idx}.wav"
        chunk_audio.export(chunk_path, format="wav")
        
        try:
            # æ›¸ãèµ·ã“ã—
            output = model.transcribe([str(chunk_path)], timestamps=True)
            
            if output and output[0]:
                if hasattr(output[0], 'timestamp') and output[0].timestamp and 'segment' in output[0].timestamp:
                    chunk_segments = output[0].timestamp['segment']
                    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’åŠ ç®—
                    for seg in chunk_segments:
                        seg['start'] += chunk_start
                        seg['end'] += chunk_start
                    all_segments.extend(chunk_segments)
                else:
                    text = output[0].text if hasattr(output[0], 'text') else str(output[0])
                    all_segments.append({'start': chunk_start, 'end': chunk_end, 'segment': text})
        finally:
            # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            if chunk_path.exists():
                chunk_path.unlink()
            # GPUãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
            gc.collect()
            if device == "cuda":
                torch.cuda.empty_cache()
        
        chunk_start = chunk_end
        chunk_idx += 1
    
    return all_segments


def save_transcript_files(segments: list, audio_name: str) -> tuple:
    """
    æ›¸ãèµ·ã“ã—çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    
    Args:
        segments: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        audio_name: ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰
    
    Returns:
        (full_text, txt_path, srt_path, csv_path)
    """
    full_text = "".join([s['segment'] for s in segments])
    
    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
    txt_path = TEMP_DIR / f"{audio_name}_transcript.txt"
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write(full_text)
    
    # SRTãƒ•ã‚¡ã‚¤ãƒ«
    srt_path = TEMP_DIR / f"{audio_name}_transcript.srt"
    with open(srt_path, 'w', encoding='utf-8') as f:
        for i, s in enumerate(segments):
            f.write(f"{i+1}\n")
            f.write(f"{format_srt_time(s['start'])} --> {format_srt_time(s['end'])}\n")
            f.write(f"{s['segment']}\n\n")
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«
    csv_path = TEMP_DIR / f"{audio_name}_transcript.csv"
    with open(csv_path, 'w', encoding='utf-8') as f:
        f.write("é–‹å§‹,çµ‚äº†,ãƒ†ã‚­ã‚¹ãƒˆ\n")
        for s in segments:
            f.write(f"{s['start']:.2f},{s['end']:.2f},\"{s['segment']}\"\n")
    
    return full_text, str(txt_path), str(srt_path), str(csv_path)


def transcribe_audio(audio_input):
    """
    éŸ³å£°/å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãèµ·ã“ã—ï¼ˆãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼‰
    
    å‡¦ç†ãƒ•ãƒ­ãƒ¼:
    1. å‹•ç”»ã®å ´åˆã¯éŸ³å£°ã‚’æŠ½å‡º
    2. 16kHz ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
    3. 5åˆ†ä»¥ä¸Šã®å ´åˆã¯ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å‡¦ç†
    4. çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    """
    if audio_input is None:
        return "âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„", None, None, None
    
    # ãƒ‘ã‚¹å–å¾—
    if hasattr(audio_input, 'name'):
        audio_path = audio_input.name
    else:
        audio_path = str(audio_input)
    
    audio_name = Path(audio_path).stem
    
    try:
        # ========================================
        # 1. å‹•ç”»ã®å ´åˆã¯éŸ³å£°ã‚’æŠ½å‡º
        # ========================================
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".webm", ".flv", ".wmv"}
        if Path(audio_path).suffix.lower() in video_extensions:
            print(f"ğŸ¬ å‹•ç”»ã‹ã‚‰éŸ³å£°ã‚’æŠ½å‡ºä¸­...")
            temp_audio = TEMP_DIR / f"{audio_name}_extracted.wav"
            if not extract_audio_from_video(audio_path, str(temp_audio)):
                return "âŒ å‹•ç”»ã‹ã‚‰ã®éŸ³å£°æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ", None, None, None
            audio_path = str(temp_audio)
        
        # ========================================
        # 2. éŸ³å£°ã‚’16kHz ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
        # ========================================
        print(f"ğŸµ éŸ³å£°ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        audio = AudioSegment.from_file(audio_path)
        duration_sec = audio.duration_seconds
        
        if audio.frame_rate != 16000:
            audio = audio.set_frame_rate(16000)
        if audio.channels != 1:
            audio = audio.set_channels(1)
        
        processed_path = TEMP_DIR / f"{audio_name}_processed.wav"
        audio.export(processed_path, format="wav")
        
        # ========================================
        # 3. æ›¸ãèµ·ã“ã—ï¼ˆé•·ã•ã«å¿œã˜ã¦å‡¦ç†ã‚’åˆ†å²ï¼‰
        # ========================================
        print(f"ğŸ“ æ›¸ãèµ·ã“ã—ä¸­... ({duration_sec:.1f}ç§’)")
        
        MAX_CHUNK_SEC = 300  # 5åˆ†
        
        if duration_sec > MAX_CHUNK_SEC:
            # é•·ã„éŸ³å£°: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å‡¦ç†
            segments = transcribe_long_audio(audio, audio_name, duration_sec)
        else:
            # çŸ­ã„éŸ³å£°: ãã®ã¾ã¾å‡¦ç†
            segments = transcribe_short_audio(str(processed_path), duration_sec)
        
        if not segments:
            return "âŒ æ›¸ãèµ·ã“ã—ã«å¤±æ•—ã—ã¾ã—ãŸ", None, None, None
        
        print(f"âœ… å®Œäº†ï¼ {len(segments)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
        
        # ========================================
        # 4. çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        # ========================================
        full_text, txt_path, srt_path, csv_path = save_transcript_files(segments, audio_name)
        print(f"   {len(full_text)}æ–‡å­—")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if processed_path.exists():
            processed_path.unlink()
        
        return full_text, txt_path, srt_path, csv_path
    
    except Exception as e:
        print(f"Error: {e}")
        return f"âŒ ã‚¨ãƒ©ãƒ¼: {e}", None, None, None


# ========================================
# Gradio UI
# ========================================
with gr.Blocks(title="ğŸ¦œ Parakeet æ—¥æœ¬èªéŸ³å£°æ›¸ãèµ·ã“ã—") as demo:
    
    gr.Markdown("""
    # ğŸ¦œ Parakeet TDT-CTC 0.6B-ja
    ## æ—¥æœ¬èªéŸ³å£°æ›¸ãèµ·ã“ã—ãƒ„ãƒ¼ãƒ«
    """)
    
    file_input = gr.File(
        label="éŸ³å£°/å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        file_types=[".mp4", ".mkv", ".avi", ".mov", ".webm", ".wav", ".mp3", ".flac", ".m4a"]
    )
    
    transcribe_btn = gr.Button("ğŸ™ï¸ æ›¸ãèµ·ã“ã—é–‹å§‹", variant="primary", size="lg")
    
    gr.Markdown("---")
    gr.Markdown("### ğŸ“„ çµæœ")
    
    result_text = gr.Textbox(label="æ›¸ãèµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆ", lines=10)
    
    with gr.Row():
        txt_file = gr.File(label="ğŸ“¥ ãƒ†ã‚­ã‚¹ãƒˆ")
        srt_file = gr.File(label="ğŸ“¥ SRTå­—å¹•")
        csv_file = gr.File(label="ğŸ“¥ CSV")
    
    transcribe_btn.click(
        fn=transcribe_audio,
        inputs=[file_input],
        outputs=[result_text, txt_file, srt_file, csv_file]
    )


if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¦œ Parakeet æ—¥æœ¬èªéŸ³å£°æ›¸ãèµ·ã“ã— Web UI (ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ)")
    print("=" * 60)
    
    demo.launch(server_name="0.0.0.0", server_port=7860)
