#!/usr/bin/env python3
"""
ğŸ¦œ Parakeet TDT-CTC 0.6B-ja Web UI (ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ)
"""

# CUDAã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå•é¡Œã‚’å›é¿
import multiprocessing
try:
    multiprocessing.set_start_method('spawn', force=True)
except RuntimeError:
    pass

from nemo.collections.asr.models import ASRModel
import torch
import gradio as gr
import gc
import shutil
from pathlib import Path
from pydub import AudioSegment
import os
import subprocess
import datetime

# ========================================
# è¨­å®š
# ========================================
SCRIPT_DIR = Path(__file__).parent.resolve()
MODEL_PATH = SCRIPT_DIR / "parakeet-tdt_ctc-0.6b-ja.nemo"
TEMP_DIR = SCRIPT_DIR / "temp"
TEMP_DIR.mkdir(exist_ok=True)

# GPUã‚’ä½¿ç”¨
device = "cuda" if torch.cuda.is_available() else "cpu"

# ãƒ¢ãƒ‡ãƒ«ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«èª­ã¿è¾¼ã¿
print(f"ğŸ¦œ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {MODEL_PATH}")
print(f"   ãƒ‡ãƒã‚¤ã‚¹: {device}")

if not MODEL_PATH.exists():
    raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {MODEL_PATH}")

model = ASRModel.restore_from(str(MODEL_PATH))
model.eval()

# GPUã«ç§»å‹•
if device == "cuda":
    model = model.cuda()
    # WSL2äº’æ›æ€§ã®ãŸã‚CTCãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨ï¼ˆCUDA Graphsã‚’å›é¿ï¼‰
    model.cur_decoder = "ctc"
    print("   ãƒ¢ãƒ‡ãƒ«ã‚’GPUã«ç§»å‹•ã—ã¾ã—ãŸ (CTCãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ä½¿ç”¨)")
else:
    print("   CPUãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œä¸­")

print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†\n")


def extract_audio_from_video(video_path: str, output_path: str) -> bool:
    """å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰éŸ³å£°ã‚’æŠ½å‡º"""
    try:
        cmd = [
            "ffmpeg", "-y", "-i", video_path,
            "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1",
            output_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        return result.returncode == 0
    except Exception as e:
        print(f"FFmpeg error: {e}")
        return False


def format_srt_time(seconds: float) -> str:
    """ç§’æ•°ã‚’SRTå½¢å¼ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«å¤‰æ›"""
    delta = datetime.timedelta(seconds=max(0.0, seconds))
    total_int_seconds = int(delta.total_seconds())
    hours = total_int_seconds // 3600
    minutes = (total_int_seconds % 3600) // 60
    seconds_part = total_int_seconds % 60
    milliseconds = delta.microseconds // 1000
    return f"{hours:02d}:{minutes:02d}:{seconds_part:02d},{milliseconds:03d}"


def transcribe_audio(audio_input):
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãèµ·ã“ã—"""
    if audio_input is None:
        return "âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„", None, None, None
    
    # ãƒ‘ã‚¹å–å¾—
    if hasattr(audio_input, 'name'):
        audio_path = audio_input.name
    else:
        audio_path = str(audio_input)
    
    audio_name = Path(audio_path).stem
    
    try:
        # å‹•ç”»ã®å ´åˆã¯éŸ³å£°æŠ½å‡º
        video_extensions = {".mp4", ".mkv", ".avi", ".mov", ".webm", ".flv", ".wmv"}
        if Path(audio_path).suffix.lower() in video_extensions:
            print(f"ğŸ¬ å‹•ç”»ã‹ã‚‰éŸ³å£°ã‚’æŠ½å‡ºä¸­...")
            temp_audio = TEMP_DIR / f"{audio_name}_extracted.wav"
            if not extract_audio_from_video(audio_path, str(temp_audio)):
                return "âŒ å‹•ç”»ã‹ã‚‰ã®éŸ³å£°æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ", None, None, None
            audio_path = str(temp_audio)
        
        # éŸ³å£°èª­ã¿è¾¼ã¿ãƒ»å¤‰æ›
        print(f"ğŸµ éŸ³å£°ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        audio = AudioSegment.from_file(audio_path)
        duration_sec = audio.duration_seconds
        
        # 16kHz ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
        if audio.frame_rate != 16000:
            audio = audio.set_frame_rate(16000)
        if audio.channels != 1:
            audio = audio.set_channels(1)
        
        processed_path = TEMP_DIR / f"{audio_name}_processed.wav"
        audio.export(processed_path, format="wav")
        
        # æ›¸ãèµ·ã“ã—
        print(f"ğŸ“ æ›¸ãèµ·ã“ã—ä¸­... ({duration_sec:.1f}ç§’)")
        
        # é•·ã„éŸ³å£°ã®å ´åˆã¯åˆ†å‰²å‡¦ç†ï¼ˆ5åˆ†ã”ã¨ï¼‰
        MAX_CHUNK_SEC = 300  # 5åˆ†
        
        if duration_sec > MAX_CHUNK_SEC:
            print(f"âš¡ é•·ã„éŸ³å£°ã®ãŸã‚{MAX_CHUNK_SEC}ç§’ã”ã¨ã«åˆ†å‰²å‡¦ç†...")
            all_segments = []
            chunk_start = 0
            chunk_idx = 0
            
            while chunk_start < duration_sec:
                chunk_end = min(chunk_start + MAX_CHUNK_SEC, duration_sec)
                print(f"   ãƒãƒ£ãƒ³ã‚¯ {chunk_idx + 1}: {chunk_start:.0f}ç§’ - {chunk_end:.0f}ç§’")
                
                # ãƒãƒ£ãƒ³ã‚¯ã‚’åˆ‡ã‚Šå‡ºã—
                chunk_audio = audio[int(chunk_start * 1000):int(chunk_end * 1000)]
                chunk_path = TEMP_DIR / f"{audio_name}_chunk_{chunk_idx}.wav"
                chunk_audio.export(chunk_path, format="wav")
                
                # æ›¸ãèµ·ã“ã—
                try:
                    output = model.transcribe([str(chunk_path)], timestamps=True)
                    
                    if output and output[0]:
                        if hasattr(output[0], 'timestamp') and output[0].timestamp and 'segment' in output[0].timestamp:
                            chunk_segments = output[0].timestamp['segment']
                            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ã‚ªãƒ•ã‚»ãƒƒãƒˆ
                            for seg in chunk_segments:
                                seg['start'] += chunk_start
                                seg['end'] += chunk_start
                            all_segments.extend(chunk_segments)
                        else:
                            text = output[0].text if hasattr(output[0], 'text') else str(output[0])
                            all_segments.append({'start': chunk_start, 'end': chunk_end, 'segment': text})
                finally:
                    # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                    if chunk_path.exists():
                        chunk_path.unlink()
                    # GPUãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
                    gc.collect()
                    if device == "cuda":
                        torch.cuda.empty_cache()
                
                chunk_start = chunk_end
                chunk_idx += 1
            
            segments = all_segments
        else:
            # çŸ­ã„éŸ³å£°ã¯ãã®ã¾ã¾å‡¦ç†
            output = model.transcribe([str(processed_path)], timestamps=True)
            
            if not output or not output[0]:
                return "âŒ æ›¸ãèµ·ã“ã—ã«å¤±æ•—ã—ã¾ã—ãŸ", None, None, None
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å–å¾—
            if hasattr(output[0], 'timestamp') and output[0].timestamp and 'segment' in output[0].timestamp:
                segments = output[0].timestamp['segment']
            else:
                text = output[0].text if hasattr(output[0], 'text') else str(output[0])
                segments = [{'start': 0.0, 'end': duration_sec, 'segment': text}]
        
        # ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
        full_text = "".join([s['segment'] for s in segments])
        print(f"âœ… å®Œäº†ï¼ {len(segments)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ, {len(full_text)}æ–‡å­—")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        txt_path = TEMP_DIR / f"{audio_name}_transcript.txt"
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(full_text)
        
        srt_path = TEMP_DIR / f"{audio_name}_transcript.srt"
        with open(srt_path, 'w', encoding='utf-8') as f:
            for i, s in enumerate(segments):
                f.write(f"{i+1}\n")
                f.write(f"{format_srt_time(s['start'])} --> {format_srt_time(s['end'])}\n")
                f.write(f"{s['segment']}\n\n")
        
        csv_path = TEMP_DIR / f"{audio_name}_transcript.csv"
        with open(csv_path, 'w', encoding='utf-8') as f:
            f.write("é–‹å§‹,çµ‚äº†,ãƒ†ã‚­ã‚¹ãƒˆ\n")
            for s in segments:
                f.write(f"{s['start']:.2f},{s['end']:.2f},\"{s['segment']}\"\n")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if processed_path.exists():
            processed_path.unlink()
        
        return full_text, str(txt_path), str(srt_path), str(csv_path)
    
    except Exception as e:
        print(f"Error: {e}")
        return f"âŒ ã‚¨ãƒ©ãƒ¼: {e}", None, None, None


# ========================================
# Gradio UI
# ========================================
with gr.Blocks(title="ğŸ¦œ Parakeet æ—¥æœ¬èªéŸ³å£°æ›¸ãèµ·ã“ã—") as demo:
    
    gr.Markdown("""
    # ğŸ¦œ Parakeet TDT-CTC 0.6B-ja
    ## æ—¥æœ¬èªéŸ³å£°æ›¸ãèµ·ã“ã—ãƒ„ãƒ¼ãƒ«
    """)
    
    file_input = gr.File(
        label="éŸ³å£°/å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        file_types=[".mp4", ".mkv", ".avi", ".mov", ".webm", ".wav", ".mp3", ".flac", ".m4a"]
    )
    
    transcribe_btn = gr.Button("ğŸ™ï¸ æ›¸ãèµ·ã“ã—é–‹å§‹", variant="primary", size="lg")
    
    gr.Markdown("---")
    gr.Markdown("### ğŸ“„ çµæœ")
    
    result_text = gr.Textbox(label="æ›¸ãèµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆ", lines=10)
    
    with gr.Row():
        txt_file = gr.File(label="ğŸ“¥ ãƒ†ã‚­ã‚¹ãƒˆ")
        srt_file = gr.File(label="ğŸ“¥ SRTå­—å¹•")
        csv_file = gr.File(label="ğŸ“¥ CSV")
    
    transcribe_btn.click(
        fn=transcribe_audio,
        inputs=[file_input],
        outputs=[result_text, txt_file, srt_file, csv_file]
    )


if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¦œ Parakeet æ—¥æœ¬èªéŸ³å£°æ›¸ãèµ·ã“ã— Web UI (ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ)")
    print("=" * 60)
    print("\nğŸŒ http://localhost:7860\n")
    
    demo.launch(server_name="0.0.0.0", server_port=7860)
